% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/workflow_airflow.R
\name{tuning_config}
\alias{tuning_config}
\title{Export Airflow tuning config from a HyperparameterTuner}
\usage{
tuning_config(
  tuner,
  inputs,
  job_name = NULL,
  include_cls_metadata = FALSE,
  mini_batch_size = NULL
)
}
\arguments{
\item{tuner}{(sagemaker.tuner.HyperparameterTuner): The tuner to export tuning
config from.}

\item{inputs}{: Information about the training data. Please refer to the ``fit()``
method of the associated estimator in the tuner, as this can take any of the
following forms:
* (str) - The S3 location where training data is saved.
* (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput]) - If using multiple
channels for training data, you can specify a dict mapping channel names to
strings or :func:`~sagemaker.inputs.TrainingInput` objects.
* (sagemaker.inputs.TrainingInput) - Channel configuration for S3 data sources that can
provide additional information about the training dataset. See
:func:`sagemaker.inputs.TrainingInput` for full details.
* (sagemaker.amazon.amazon_estimator.RecordSet) - A collection of
Amazon :class:~`Record` objects serialized and stored in S3.
For use with an estimator for an Amazon algorithm.
* (list[sagemaker.amazon.amazon_estimator.RecordSet]) - A list of
:class:~`sagemaker.amazon.amazon_estimator.RecordSet` objects,
where each instance is a different channel of training data.
* (dict[str, one the forms above]): Required by only tuners created via
the factory method ``HyperparameterTuner.create()``. The keys should be the
same estimator names as keys for the ``estimator_list`` argument of the
``HyperparameterTuner.create()`` method.}

\item{job_name}{(str): Specify a tuning job name if needed.}

\item{include_cls_metadata}{: It can take one of the following two forms.
* (bool) - Whether or not the hyperparameter tuning job should include information
about the estimator class (default: False). This information is passed as a
hyperparameter, so if the algorithm you are using cannot handle unknown
hyperparameters (e.g. an Amazon SageMaker built-in algorithm that does not
have a custom estimator in the Python SDK), then set ``include_cls_metadata``
to ``False``.
* (dict[str, bool]) - This version should be used for tuners created via the factory
method ``HyperparameterTuner.create()``, to specify the flag for individual
estimators provided in the ``estimator_list`` argument of the method. The keys
would be the same estimator names as in ``estimator_list``. If one estimator
doesn't need the flag set, then no need to include it in the dictionary. If none
of the estimators need the flag set, then an empty dictionary ``{}`` must be used.}

\item{mini_batch_size}{: It can take one of the following two forms.
* (int) - Specify this argument only when estimator is a built-in estimator of an
Amazon algorithm. For other estimators, batch size should be specified in the
estimator.
* (dict[str, int]) - This version should be used for tuners created via the factory
method ``HyperparameterTuner.create()``, to specify the value for individual
estimators provided in the ``estimator_list`` argument of the method. The keys
would be the same estimator names as in ``estimator_list``. If one estimator
doesn't need the value set, then no need to include it in the dictionary. If
none of the estimators need the value set, then an empty dictionary ``{}``
must be used.}
}
\value{
list: Tuning config that can be directly used by SageMakerTuningOperator in Airflow.
}
\description{
Export Airflow tuning config from a HyperparameterTuner
}
